diff --git a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index d54ed3d..dacb13c 100644
--- a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -51,6 +51,16 @@
 #include "vxlan.h"
 #endif
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+/*
+ * mlx5_netmap_linux.h contains functions for netmap support
+ * that extend the standard driver.
+ */
+#define NETMAP_MLX5_MAIN
+#define DEV_NETMAP
+#include "mlx5_netmap_linux.h"
+#endif
+
 struct mlx5e_rq_param {
 	u32			rqc[MLX5_ST_SZ_DW(rqc)];
 	struct mlx5_wq_param	wq;
@@ -86,6 +96,9 @@ struct mlx5e_channel_param {
 
 static bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev)
 {
+#ifdef DEV_NETMAP
+	return 0;
+#endif
 	return MLX5_CAP_GEN(mdev, striding_rq) &&
 		MLX5_CAP_GEN(mdev, umr_ptr_rlky) &&
 		MLX5_CAP_ETH(mdev, reg_umr_sq);
@@ -1012,6 +1025,10 @@ static int mlx5e_alloc_rq(struct mlx5e_channel *c,
 	INIT_WORK(&rq->am.work, mlx5e_rx_am_work);
 	rq->am.mode = params->rx_cq_moderation.cq_period_mode;
 
+#ifdef DEV_NETMAP
+	mlx5e_netmap_configure_rx_ring(rq, rq->ix);
+#endif /* DEV_NETMAP */
+
 	return 0;
 
 err_free:
@@ -1219,6 +1236,11 @@ static int mlx5e_wait_for_min_rx_wqes(struct mlx5e_rq *rq)
 	unsigned long exp_time = jiffies + msecs_to_jiffies(20000);
 	struct mlx5e_channel *c = rq->channel;
 
+#ifdef DEV_NETMAP
+	if (nm_netmap_on(NA(c->netdev)))
+		return 0; /* no need to wait when netmap has built wqes */
+#endif
+
 	u16 min_wqes = mlx5_min_rx_wqes(rq->wq_type, mlx5e_rqwq_get_size(rq));
 
 	while (time_before(jiffies, exp_time)) {
@@ -1226,6 +1248,10 @@ static int mlx5e_wait_for_min_rx_wqes(struct mlx5e_rq *rq)
 			return 0;
 
 		msleep(20);
+#ifdef DEV_NETMAP
+		if (nm_netmap_on(NA(c->netdev)))
+			mlx5e_netmap_rx_flush(rq); /* handle the CQEs */
+#endif
 	}
 
 	netdev_warn(c->netdev, "Failed to get min RX wqes on RQN[0x%x] wq cur_sz(%d) min_rx_wqes(%d)\n",
@@ -1251,6 +1277,9 @@ static void mlx5e_free_rx_descs(struct mlx5e_rq *rq)
 			wqe_ix_be = *wq->tail_next;
 			wqe_ix    = be16_to_cpu(wqe_ix_be);
 			wqe       = mlx5_wq_ll_get_wqe(wq, wqe_ix);
+#ifdef DEV_NETMAP
+			if (!nm_netmap_on(NA(rq->channel->netdev)))
+#endif
 			rq->dealloc_wqe(rq, wqe_ix);
 			mlx5_wq_ll_pop(wq, wqe_ix_be,
 				       &wqe->next.next_wqe_index);
@@ -1368,6 +1397,9 @@ static void mlx5e_activate_rq(struct mlx5e_rq *rq)
 	u16 pi = sq->pc & sq->wq.sz_m1;
 	struct mlx5e_tx_wqe *nopwqe;
 
+#ifdef DEV_NETMAP
+	if (!nm_netmap_on(NA(rq->channel->netdev)))
+#endif
 	set_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
 	sq->db.ico_wqe[pi].opcode     = MLX5_OPCODE_NOP;
 	nopwqe = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
@@ -1564,6 +1596,11 @@ static int mlx5e_alloc_txqsq(struct mlx5e_channel *c,
 
 	sq->edge = (sq->wq.sz_m1 + 1) - MLX5_SEND_WQE_MAX_WQEBBS;
 
+#ifdef DEV_NETMAP
+	if (mlx5e_netmap_configure_tx_ring(c->priv, txq_ix))
+		return 0;
+#endif /* DEV_NETMAP */
+
 	return 0;
 
 err_sq_wq_destroy:
@@ -1771,6 +1808,9 @@ static void mlx5e_deactivate_txqsq(struct mlx5e_txqsq *sq)
 	netif_tx_disable_queue(sq->txq);
 
 	/* last doorbell out, godspeed .. */
+#ifdef DEV_NETMAP
+	if (!nm_netmap_on(NA(sq->txq->dev)))
+#endif
 	if (mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, 1)) {
 		struct mlx5e_tx_wqe *nop;
 
@@ -1786,11 +1826,18 @@ static void mlx5e_close_txqsq(struct mlx5e_txqsq *sq)
 	struct mlx5_core_dev *mdev = c->mdev;
 	struct mlx5_rate_limit rl = {0};
 
+
+#ifdef DEV_NETMAP
+	if (nm_netmap_on(NA(sq->txq->dev)))
+		mlx5e_netmap_tx_flush(sq); /* handle any CQEs */
+#endif
+
 	mlx5e_destroy_sq(mdev, sq->sqn);
 	if (sq->rate_limit) {
 		rl.rate = sq->rate_limit;
 		mlx5_rl_remove_rate(mdev, &rl);
 	}
+
 	mlx5e_free_txqsq_descs(sq);
 	mlx5e_free_txqsq(sq);
 }
@@ -1804,6 +1851,10 @@ static int mlx5e_wait_for_sq_flush(struct mlx5e_txqsq *sq)
 			return 0;
 
 		msleep(20);
+#ifdef DEV_NETMAP
+		if (nm_netmap_on(NA(sq->txq->dev)))
+			mlx5e_netmap_tx_flush(sq); /* handle any CQEs */
+#endif
 	}
 
 	netdev_err(sq->channel->netdev,
@@ -3532,6 +3583,10 @@ int mlx5e_open_locked(struct net_device *netdev)
 	if (priv->profile->update_carrier)
 		priv->profile->update_carrier(priv);
 
+#ifdef DEV_NETMAP
+	netmap_enable_all_rings(netdev); /* NOP if netmap not in use */
+#endif
+
 	if (priv->profile->update_stats)
 		queue_delayed_work(priv->wq, &priv->update_stats_work, 0);
 
@@ -3568,6 +3623,10 @@ int mlx5e_close_locked(struct net_device *netdev)
 
 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
 
+#ifdef DEV_NETMAP
+	netmap_disable_all_rings(netdev);
+#endif
+
 	if (MLX5E_GET_PFLAG(&priv->channels.params, MLX5E_PFLAG_SNIFFER)) {
 		mlx5e_sniffer_stop(priv);
 		MLX5E_SET_PFLAG(&priv->channels.params, MLX5E_PFLAG_SNIFFER, 0);
@@ -6125,6 +6184,11 @@ void mlx5e_destroy_netdev(struct mlx5e_priv *priv)
 	const struct mlx5e_profile *profile = priv->profile;
 	struct net_device *netdev = priv->netdev;
 
+#ifdef DEV_NETMAP
+	netmap_detach(netdev);
+#endif /* DEV_NETMAP */
+
+
 	destroy_workqueue(priv->wq);
 	if (profile->cleanup)
 		profile->cleanup(priv);
@@ -6237,6 +6301,11 @@ static void *mlx5e_add(struct mlx5_core_dev *mdev)
 	mlx5e_dcbnl_init_app(priv);
 #endif
 #endif
+
+#ifdef DEV_NETMAP
+	mlx5e_netmap_attach(priv);
+#endif /* DEV_NETMAP */
+
 	return priv;
 
 err_unregister_netdev:
diff --git a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 798b1de..50cdc0b 100644
--- a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -150,7 +150,7 @@ static inline u32 mlx5e_decompress_cqes_cont(struct mlx5e_rq *rq,
 	return cqe_count;
 }
 
-static inline u32 mlx5e_decompress_cqes_start(struct mlx5e_rq *rq,
+u32 mlx5e_decompress_cqes_start(struct mlx5e_rq *rq,
 					      struct mlx5e_cq *cq,
 					      int budget_rem)
 {
diff --git a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 8659e35..73dff79 100644
--- a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -38,6 +38,15 @@
 #include "en_accel/ipsec_rxtx.h"
 #include "lib/clock.h"
 
+
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+/*
+ * mlx5_netmap_linux.h contains functions for netmap support
+ * that extend the standard driver.
+ */
+#include "mlx5_netmap_linux.h"
+#endif
+
 #define MLX5E_SQ_NOPS_ROOM  MLX5_SEND_WQE_MAX_WQEBBS
 #define MLX5E_SQ_STOP_ROOM (MLX5_SEND_WQE_MAX_WQEBBS +\
 			    MLX5E_SQ_NOPS_ROOM)
@@ -688,15 +697,17 @@ void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq)
 			continue;
 		}
 
-		for (i = 0; i < wi->num_dma; i++) {
-			struct mlx5e_sq_dma *dma =
-				mlx5e_dma_get(sq, sq->dma_fifo_cc++);
+		if (!nm_netmap_on(NA(sq->txq->dev))) {
+			/* do not free skbs in netmap mode */
+			for (i = 0; i < wi->num_dma; i++) {
+				struct mlx5e_sq_dma *dma =
+					mlx5e_dma_get(sq, sq->dma_fifo_cc++);
 
-			mlx5e_tx_dma_unmap(sq->pdev, dma);
+				mlx5e_tx_dma_unmap(sq->pdev, dma);
+			}
+			dev_kfree_skb_any(skb);
 		}
-
-		dev_kfree_skb_any(skb);
-		sq->cc += wi->num_wqebbs;
+        sq->cc += wi->num_wqebbs;
 	}
 }
 
diff --git a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index b914249..c1f2b8e 100644
--- a/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/mlx5/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -35,6 +35,15 @@
 #endif
 #include "en.h"
 
+
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+/*
+ * mlx5_netmap_linux.h contains functions for netmap support
+ * that extend the standard driver.
+ */
+#include "mlx5_netmap_linux.h"
+#endif
+
 #if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 static inline bool mlx5e_channel_no_affinity_change(struct mlx5e_channel *c)
 {
@@ -64,6 +73,45 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget)
 	clear_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags);
 #endif
 
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+	if (nm_netmap_on(NA(c->netdev))) {
+		/*
+		 * In netmap mode, all the work is done in the context
+		 * of the client thread. Interrupt handlers only wake up
+		 * clients, which may be sleeping on individual rings
+		 * or on a global resource for all rings.
+		 */
+		struct mlx5e_rq *rq = &c->rq;
+		int dummy;
+
+		/* Wake netmap rx client. This results in a call to
+		 * mlx5e_netmap_rxsync() which will check for any
+		 * received packets and process them
+		 */
+		netmap_rx_irq(rq->netdev, rq->ix, &dummy);
+
+		for (i = 0; i < c->num_tc; i++) {
+
+			struct mlx5e_cq *scq = &c->sq[i].cq;
+
+			/* Wake netmap tx client. This results in a call to
+			 * mlx5e_netmap_txsync()  which will check if a batch
+			 * of packets has finished sending and recycle the
+			 * buffers
+			 */
+			netmap_tx_irq(scq->channel->netdev, scq->channel->ix);
+		}
+
+		/* cq interrupts are not re-armed until the end of the
+		 * mlx5e_netmap_*sync() functions so we don't get more
+		 * interrupts if a call to those is already pending or in progress.
+		 */
+		napi_complete(napi);
+
+		return 0;
+	}
+#endif
+
 	for (i = 0; i < c->num_tc; i++)
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq, budget);
 
